"""
This is an entry point, no other file should import from this one.
Train a model to return a Roman Numeral analysis given a score in musicxml format.
"""
import os
from argparse import ArgumentParser
from datetime import datetime

import seaborn as sns
from matplotlib import pyplot as plt
from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint

from frog import INPUT_TYPES
from frog.load_data import load_tfrecords_dataset
from frog.models_old import create_model

# TODO: This module is currently broken! Should we just remove it?

batch_size = 16
shuffle_buffer = 100_000
epochs = 100


def visualize_data(data):
    for x, y in data:
        prs, masks, _, _ = x
        for pr, mask in zip(prs, masks):
            sns.heatmap(pr)
            plt.show()
            plt.plot(mask.numpy())
            plt.show()
    return


def main(opts):
    models = ["conv_gru", "conv_dil", "gru", "conv_gru_local", "conv_dil_local"]
    parser = ArgumentParser(description="Train a neural network for Roman Numeral analysis")
    parser.add_argument(
        "tfrecords_folder",
        action="store",
        help="The folder where the tfrecords generated by the preprocessing module are stored",
    )
    parser.add_argument(
        "-m", dest="model_type", action="store", choices=models, help=f"Model to use"
    )
    parser.add_argument(
        "-i",
        dest="input_type",
        action="store",
        choices=INPUT_TYPES,
        help=f"Input type to use",
    )
    args = parser.parse_args(opts)
    model_type, input_type = args.model_type, args.input_type

    train_path, valid_path = [
        os.path.join(args.tfrecords_folder, f"{ds}_{input_type}.tfrecords")
        for ds in ["train", "valid"]
    ]
    train_data = load_tfrecords_dataset(
        train_path, compression, batch_size, shuffle_buffer, input_type, output_mode
    )
    valid_data = load_tfrecords_dataset(
        valid_path, compression, batch_size, 1, input_type, output_mode
    )
    n_train = sum([1 for _ in train_data.unbatch()])  # it takes a few seconds, but it's OK...
    n_valid = sum([1 for _ in valid_data.unbatch()])
    print(n_train, n_valid)
    model_name = "_".join([model_type, input_type, datetime.now().strftime("%Y-%m-%d_%H-%M-%S")])
    model_folder = os.path.join("..", "logs", model_name)
    os.makedirs(model_folder)

    model_path = os.path.join(model_folder, model_name)
    model = create_model(
        model_name, model_type=model_type, input_type=input_type, derive_root=False
    )
    model.summary()

    callbacks = [
        EarlyStopping(patience=3),
        TensorBoard(log_dir=model_folder),
        ModelCheckpoint(filepath=model_path, save_best_only=True),
    ]

    # weights = [1., 0.5, 1., 1., 0.5, 2.]  # [y_key, y_dg1, y_dg2, y_qlt, y_inv, y_roo]
    weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # [y_key, y_dg1, y_dg2, y_qlt, y_inv, y_roo]
    model.compile(
        loss="categorical_crossentropy",
        loss_weights=weights,
        optimizer="adam",
        metrics=["accuracy"],
    )

    history = model.fit(train_data, epochs=epochs, validation_data=valid_data, callbacks=callbacks)
    model.save(model_path)
    return history
